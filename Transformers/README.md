# Transformers
### [Binxu Wang](https://scholar.harvard.edu/binxuw) 

This session is on [**transformers**](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)), a particular machine learning architecture that has been found to be extremely good at [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), among other tasks. Transformers are the backbone of chatbots like [ChatGPT](https://openai.com/blog/chatgpt), and they have been [*so* successful](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/) that [people are at least taking seriously](https://cbmm.mit.edu/news-events/events/discussion-panel-transformers-vs-humans-ultimate-battle-general-intelligence) the possibility that they could provide the foundation for something approximating general intelligence. 

Transformers involve a specific repeated building block based on the idea of [QKV attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) (query-key-value). Theoretically, it is not totally clear why the basic motif transformers use works so well; in addition to natural language processing, transformers can [model music](https://arxiv.org/abs/1809.04281), [perform the same image recognition tasks as convolutional neural networks](https://arxiv.org/abs/2108.08810), and even serve as [one possible architecture for diffusion models](https://arxiv.org/abs/2212.09748).

In this session, we will present the basic architecture, and work through a number of applications and examples together.

**Helpful blog posts**:

- Intro to transformers | [https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)

**Relevant papers**:

[2027 Vaswani et al.] [Attention is All you Need](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)





**Links to notebooks**:

Day 1 

- 

Day 2: 

- 
