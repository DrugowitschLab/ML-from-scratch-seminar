# Transformers
### [Binxu Wang](https://scholar.harvard.edu/binxuw) 

This session is on **transformers**, a particular machine learning architecture that has been found to be extremely good at natural language processing, among other tasks. Transformers are the backbone of chatbots like Chat-GPT, and some argue that they provide a foundation for something approximating general intelligence.

Transformers involve a specific repeated building block. Theoretically, it is not totally clear why the basic motif transformers use works so well; in addition to natural language processing, transformers can model music, perform the same image recognition tasks as convolutional neural networks, and even serve as one possible architecture for diffusion models.

In this session, we will present the basic architecture, and work through a number of applications and examples together.

**Helpful blog posts**:


**Relevant papers**:


**Links to notebooks**:

Day 1 

- 

Day 2: 

- 
